{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441ce59b-adfd-458e-b138-1a85aa9c23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69188ed7-c54b-4706-9dff-381b051f336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Sonar Data CSV file into a pandas DataFrame.\n",
    "# The 'header=None' argument indicates that the dataset does not have a header row, \n",
    "# so pandas will assign numerical column labels by default (0, 1, 2, ..., n).\n",
    "# Replace the file path with your own if needed.\n",
    "df= pd.read_csv(\"C:/Users/ABHISHEK DEORE/OneDrive/Desktop/Projects/Rock VS Mine Prediction/Data/SonarData.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97395882-0d36-4a00-9f85-67f1ee384770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7363bbc2-d9d4-41c6-b050-f10ccb66af2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9       10      11      12      13      14      15      16      17  \\\n",
       "0  0.2111  0.1609  0.1582  0.2238  0.0645  0.0660  0.2273  0.3100  0.2999   \n",
       "1  0.2872  0.4918  0.6552  0.6919  0.7797  0.7464  0.9444  1.0000  0.8874   \n",
       "2  0.6194  0.6333  0.7060  0.5544  0.5320  0.6479  0.6931  0.6759  0.7551   \n",
       "3  0.1264  0.0881  0.1992  0.0184  0.2261  0.1729  0.2131  0.0693  0.2281   \n",
       "4  0.4459  0.4152  0.3952  0.4256  0.4135  0.4528  0.5326  0.7306  0.6193   \n",
       "\n",
       "       18      19      20      21      22      23      24      25      26  \\\n",
       "0  0.5078  0.4797  0.5783  0.5071  0.4328  0.5550  0.6711  0.6415  0.7104   \n",
       "1  0.8024  0.7818  0.5212  0.4052  0.3957  0.3914  0.3250  0.3200  0.3271   \n",
       "2  0.8929  0.8619  0.7974  0.6737  0.4293  0.3648  0.5331  0.2413  0.5070   \n",
       "3  0.4060  0.3973  0.2741  0.3690  0.5556  0.4846  0.3140  0.5334  0.5256   \n",
       "4  0.2032  0.4636  0.4148  0.4292  0.5730  0.5399  0.3161  0.2285  0.6995   \n",
       "\n",
       "       27      28      29      30      31      32      33      34      35  \\\n",
       "0  0.8080  0.6791  0.3857  0.1307  0.2604  0.5121  0.7547  0.8537  0.8507   \n",
       "1  0.2767  0.4423  0.2028  0.3788  0.2947  0.1984  0.2341  0.1306  0.4182   \n",
       "2  0.8533  0.6036  0.8514  0.8512  0.5045  0.1862  0.2709  0.4232  0.3043   \n",
       "3  0.2520  0.2090  0.3559  0.6260  0.7340  0.6120  0.3497  0.3953  0.3012   \n",
       "4  1.0000  0.7262  0.4724  0.5103  0.5459  0.2881  0.0981  0.1951  0.4181   \n",
       "\n",
       "       36      37      38      39      40      41      42      43      44  \\\n",
       "0  0.6692  0.6097  0.4943  0.2744  0.0510  0.2834  0.2825  0.4256  0.2641   \n",
       "1  0.3835  0.1057  0.1840  0.1970  0.1674  0.0583  0.1401  0.1628  0.0621   \n",
       "2  0.6116  0.6756  0.5375  0.4719  0.4647  0.2587  0.2129  0.2222  0.2111   \n",
       "3  0.5408  0.8814  0.9857  0.9167  0.6121  0.5006  0.3210  0.3202  0.4295   \n",
       "4  0.4604  0.3217  0.2828  0.2430  0.1979  0.2444  0.1847  0.0841  0.0692   \n",
       "\n",
       "       45      46      47      48      49      50      51      52      53  \\\n",
       "0  0.1386  0.1051  0.1343  0.0383  0.0324  0.0232  0.0027  0.0065  0.0159   \n",
       "1  0.0203  0.0530  0.0742  0.0409  0.0061  0.0125  0.0084  0.0089  0.0048   \n",
       "2  0.0176  0.1348  0.0744  0.0130  0.0106  0.0033  0.0232  0.0166  0.0095   \n",
       "3  0.3654  0.2655  0.1576  0.0681  0.0294  0.0241  0.0121  0.0036  0.0150   \n",
       "4  0.0528  0.0357  0.0085  0.0230  0.0046  0.0156  0.0031  0.0054  0.0105   \n",
       "\n",
       "       54      55      56      57      58      59 60  \n",
       "0  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032  R  \n",
       "1  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044  R  \n",
       "2  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078  R  \n",
       "3  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117  R  \n",
       "4  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094  R  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the DataFrame to get a quick overview of the data.\n",
    "# This is useful for checking if the data was loaded correctly and to inspect the first few records.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa914e2a-5ac9-4de7-b16e-fe986666fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the dimensions of the DataFrame (number of rows, number of columns).\n",
    "# This is useful for understanding the size of the dataset and ensuring it has been loaded properly.\n",
    "# The output will be in the form of (number_of_rows, number_of_columns).\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b043a0-cfbf-4d2c-a712-b0b020abb67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.263950</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.434950</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>0.490350</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.351050</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.278050</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.222550</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.121350</td>\n",
       "      <td>0.101650</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965700</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259    0.236013    0.250221   \n",
       "std      0.061788    0.085152    0.118387    0.134416    0.132705    0.140072   \n",
       "min      0.003300    0.005500    0.007500    0.011300    0.028900    0.023600   \n",
       "25%      0.080900    0.080425    0.097025    0.111275    0.129250    0.133475   \n",
       "50%      0.106950    0.112100    0.152250    0.182400    0.224800    0.249050   \n",
       "75%      0.154000    0.169600    0.233425    0.268700    0.301650    0.331250   \n",
       "max      0.372900    0.459000    0.682800    0.710600    0.734200    0.706000   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.273305    0.296568    0.320201    0.378487    0.415983    0.452318   \n",
       "std      0.140962    0.164474    0.205427    0.232650    0.263677    0.261529   \n",
       "min      0.018400    0.027300    0.003100    0.016200    0.034900    0.037500   \n",
       "25%      0.166125    0.175175    0.164625    0.196300    0.205850    0.242075   \n",
       "50%      0.263950    0.281100    0.281700    0.304700    0.308400    0.368300   \n",
       "75%      0.351250    0.386175    0.452925    0.535725    0.659425    0.679050   \n",
       "max      0.713100    0.997000    1.000000    0.998800    1.000000    1.000000   \n",
       "\n",
       "               18          19          20          21          22          23  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504812    0.563047    0.609060    0.624275    0.646975    0.672654   \n",
       "std      0.257988    0.262653    0.257818    0.255883    0.250175    0.239116   \n",
       "min      0.049400    0.065600    0.051200    0.021900    0.056300    0.023900   \n",
       "25%      0.299075    0.350625    0.399725    0.406925    0.450225    0.540725   \n",
       "50%      0.434950    0.542500    0.617700    0.664900    0.699700    0.698500   \n",
       "75%      0.731400    0.809325    0.816975    0.831975    0.848575    0.872175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               24          25          26          27          28          29  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.675424    0.699866    0.702155    0.694024    0.642074    0.580928   \n",
       "std      0.244926    0.237228    0.245657    0.237189    0.240250    0.220749   \n",
       "min      0.024000    0.092100    0.048100    0.028400    0.014400    0.061300   \n",
       "25%      0.525800    0.544175    0.531900    0.534775    0.463700    0.411400   \n",
       "50%      0.721100    0.754500    0.745600    0.731900    0.680800    0.607150   \n",
       "75%      0.873725    0.893800    0.917100    0.900275    0.852125    0.735175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               30          31          32          33          34          35  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504475    0.439040    0.417220    0.403233    0.392571    0.384848   \n",
       "std      0.213992    0.213237    0.206513    0.231242    0.259132    0.264121   \n",
       "min      0.048200    0.040400    0.047700    0.021200    0.022300    0.008000   \n",
       "25%      0.345550    0.281400    0.257875    0.217575    0.179375    0.154350   \n",
       "50%      0.490350    0.429600    0.391200    0.351050    0.312750    0.321150   \n",
       "75%      0.641950    0.580300    0.556125    0.596125    0.593350    0.556525   \n",
       "max      0.965700    0.930600    1.000000    0.964700    1.000000    1.000000   \n",
       "\n",
       "               36          37          38          39          40          41  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.363807    0.339657    0.325800    0.311207    0.289252    0.278293   \n",
       "std      0.239912    0.212973    0.199075    0.178662    0.171111    0.168728   \n",
       "min      0.035100    0.038300    0.037100    0.011700    0.036000    0.005600   \n",
       "25%      0.160100    0.174275    0.173975    0.186450    0.163100    0.158900   \n",
       "50%      0.306300    0.312700    0.283500    0.278050    0.259500    0.245100   \n",
       "75%      0.518900    0.440550    0.434900    0.424350    0.387525    0.384250   \n",
       "max      0.949700    1.000000    0.985700    0.929700    0.899500    0.824600   \n",
       "\n",
       "               42          43          44          45          46          47  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.246542    0.214075    0.197232    0.160631    0.122453    0.091424   \n",
       "std      0.138993    0.133291    0.151628    0.133938    0.086953    0.062417   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.155200    0.126875    0.094475    0.068550    0.064250    0.045125   \n",
       "50%      0.222550    0.177700    0.148000    0.121350    0.101650    0.078100   \n",
       "75%      0.324525    0.271750    0.231550    0.200375    0.154425    0.120100   \n",
       "max      0.773300    0.776200    0.703400    0.729200    0.552200    0.333900   \n",
       "\n",
       "               48          49          50          51          52          53  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.051929    0.020424    0.016069    0.013420    0.010709    0.010941   \n",
       "std      0.035954    0.013665    0.012008    0.009634    0.007060    0.007301   \n",
       "min      0.000000    0.000000    0.000000    0.000800    0.000500    0.001000   \n",
       "25%      0.026350    0.011550    0.008425    0.007275    0.005075    0.005375   \n",
       "50%      0.044700    0.017900    0.013900    0.011400    0.009550    0.009300   \n",
       "75%      0.068525    0.025275    0.020825    0.016725    0.014900    0.014500   \n",
       "max      0.198100    0.082500    0.100400    0.070900    0.039000    0.035200   \n",
       "\n",
       "               54          55          56          57          58          59  \n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000  \n",
       "mean     0.009290    0.008222    0.007820    0.007949    0.007941    0.006507  \n",
       "std      0.007088    0.005736    0.005785    0.006470    0.006181    0.005031  \n",
       "min      0.000600    0.000400    0.000300    0.000300    0.000100    0.000600  \n",
       "25%      0.004150    0.004400    0.003700    0.003600    0.003675    0.003100  \n",
       "50%      0.007500    0.006850    0.005950    0.005800    0.006400    0.005300  \n",
       "75%      0.012100    0.010575    0.010425    0.010350    0.010325    0.008525  \n",
       "max      0.044700    0.039400    0.035500    0.044000    0.036400    0.043900  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating descriptive statistics for the DataFrame.\n",
    "# This method provides a summary of the central tendency, dispersion, and shape of the dataset's distribution.\n",
    "# It includes metrics such as count, mean, standard deviation, minimum, maximum, and the 25th, 50th, and 75th percentiles.\n",
    "# It is useful for quickly understanding the statistical properties of the numerical data.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ca5f6d-0380-467f-9029-085be98ffe6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Displaying a concise summary of the DataFrame.\n",
    "# This includes information about the number of non-null entries, data types of each column, \n",
    "# and memory usage of the DataFrame. It is useful for getting a quick overview of the structure of the dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918f2cd9-9edc-4b74-a27c-6d37f188e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60\n",
       "M    111\n",
       "R     97\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the frequency of unique values in column 60 of the DataFrame.\n",
    "# In this dataset, column 60 likely represents the target variable (e.g., rock or mine).\n",
    "# The method will return the count of occurrences for each unique value in the column, \n",
    "# which is useful for understanding the class distribution of the target variable.\n",
    "df[60].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9804da8d-2299-4722-9cb9-46bd434ca0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
      "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
      "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
      "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
      "\n",
      "         9       10      11      12      13      14      15      16      17  \\\n",
      "0    0.2111  0.1609  0.1582  0.2238  0.0645  0.0660  0.2273  0.3100  0.2999   \n",
      "1    0.2872  0.4918  0.6552  0.6919  0.7797  0.7464  0.9444  1.0000  0.8874   \n",
      "2    0.6194  0.6333  0.7060  0.5544  0.5320  0.6479  0.6931  0.6759  0.7551   \n",
      "3    0.1264  0.0881  0.1992  0.0184  0.2261  0.1729  0.2131  0.0693  0.2281   \n",
      "4    0.4459  0.4152  0.3952  0.4256  0.4135  0.4528  0.5326  0.7306  0.6193   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.2684  0.3108  0.2933  0.2275  0.0994  0.1801  0.2200  0.2732  0.2862   \n",
      "204  0.2154  0.3085  0.3425  0.2990  0.1402  0.1235  0.1534  0.1901  0.2429   \n",
      "205  0.2529  0.2716  0.2374  0.1878  0.0983  0.0683  0.1503  0.1723  0.2339   \n",
      "206  0.2354  0.2898  0.2812  0.1578  0.0273  0.0673  0.1444  0.2070  0.2645   \n",
      "207  0.2354  0.2720  0.2442  0.1665  0.0336  0.1302  0.1708  0.2177  0.3175   \n",
      "\n",
      "         18      19      20      21      22      23      24      25      26  \\\n",
      "0    0.5078  0.4797  0.5783  0.5071  0.4328  0.5550  0.6711  0.6415  0.7104   \n",
      "1    0.8024  0.7818  0.5212  0.4052  0.3957  0.3914  0.3250  0.3200  0.3271   \n",
      "2    0.8929  0.8619  0.7974  0.6737  0.4293  0.3648  0.5331  0.2413  0.5070   \n",
      "3    0.4060  0.3973  0.2741  0.3690  0.5556  0.4846  0.3140  0.5334  0.5256   \n",
      "4    0.2032  0.4636  0.4148  0.4292  0.5730  0.5399  0.3161  0.2285  0.6995   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.2034  0.1740  0.4130  0.6879  0.8120  0.8453  0.8919  0.9300  0.9987   \n",
      "204  0.2120  0.2395  0.3272  0.5949  0.8302  0.9045  0.9888  0.9912  0.9448   \n",
      "205  0.1962  0.1395  0.3164  0.5888  0.7631  0.8473  0.9424  0.9986  0.9699   \n",
      "206  0.2828  0.4293  0.5685  0.6990  0.7246  0.7622  0.9242  1.0000  0.9979   \n",
      "207  0.3714  0.4552  0.5700  0.7397  0.8062  0.8837  0.9432  1.0000  0.9375   \n",
      "\n",
      "         27      28      29      30      31      32      33      34      35  \\\n",
      "0    0.8080  0.6791  0.3857  0.1307  0.2604  0.5121  0.7547  0.8537  0.8507   \n",
      "1    0.2767  0.4423  0.2028  0.3788  0.2947  0.1984  0.2341  0.1306  0.4182   \n",
      "2    0.8533  0.6036  0.8514  0.8512  0.5045  0.1862  0.2709  0.4232  0.3043   \n",
      "3    0.2520  0.2090  0.3559  0.6260  0.7340  0.6120  0.3497  0.3953  0.3012   \n",
      "4    1.0000  0.7262  0.4724  0.5103  0.5459  0.2881  0.0981  0.1951  0.4181   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  1.0000  0.8104  0.6199  0.6041  0.5547  0.4160  0.1472  0.0849  0.0608   \n",
      "204  1.0000  0.9092  0.7412  0.7691  0.7117  0.5304  0.2131  0.0928  0.1297   \n",
      "205  1.0000  0.8630  0.6979  0.7717  0.7305  0.5197  0.1786  0.1098  0.1446   \n",
      "206  0.8297  0.7032  0.7141  0.6893  0.4961  0.2584  0.0969  0.0776  0.0364   \n",
      "207  0.7603  0.7123  0.8358  0.7622  0.4567  0.1715  0.1549  0.1641  0.1869   \n",
      "\n",
      "         36      37      38      39      40      41      42      43      44  \\\n",
      "0    0.6692  0.6097  0.4943  0.2744  0.0510  0.2834  0.2825  0.4256  0.2641   \n",
      "1    0.3835  0.1057  0.1840  0.1970  0.1674  0.0583  0.1401  0.1628  0.0621   \n",
      "2    0.6116  0.6756  0.5375  0.4719  0.4647  0.2587  0.2129  0.2222  0.2111   \n",
      "3    0.5408  0.8814  0.9857  0.9167  0.6121  0.5006  0.3210  0.3202  0.4295   \n",
      "4    0.4604  0.3217  0.2828  0.2430  0.1979  0.2444  0.1847  0.0841  0.0692   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.0969  0.1411  0.1676  0.1200  0.1201  0.1036  0.1977  0.1339  0.0902   \n",
      "204  0.1159  0.1226  0.1768  0.0345  0.1562  0.0824  0.1149  0.1694  0.0954   \n",
      "205  0.1066  0.1440  0.1929  0.0325  0.1490  0.0328  0.0537  0.1309  0.0910   \n",
      "206  0.1572  0.1823  0.1349  0.0849  0.0492  0.1367  0.1552  0.1548  0.1319   \n",
      "207  0.2655  0.1713  0.0959  0.0768  0.0847  0.2076  0.2505  0.1862  0.1439   \n",
      "\n",
      "         45      46      47      48      49      50      51      52      53  \\\n",
      "0    0.1386  0.1051  0.1343  0.0383  0.0324  0.0232  0.0027  0.0065  0.0159   \n",
      "1    0.0203  0.0530  0.0742  0.0409  0.0061  0.0125  0.0084  0.0089  0.0048   \n",
      "2    0.0176  0.1348  0.0744  0.0130  0.0106  0.0033  0.0232  0.0166  0.0095   \n",
      "3    0.3654  0.2655  0.1576  0.0681  0.0294  0.0241  0.0121  0.0036  0.0150   \n",
      "4    0.0528  0.0357  0.0085  0.0230  0.0046  0.0156  0.0031  0.0054  0.0105   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.1085  0.1521  0.1363  0.0858  0.0290  0.0203  0.0116  0.0098  0.0199   \n",
      "204  0.0080  0.0790  0.1255  0.0647  0.0179  0.0051  0.0061  0.0093  0.0135   \n",
      "205  0.0757  0.1059  0.1005  0.0535  0.0235  0.0155  0.0160  0.0029  0.0051   \n",
      "206  0.0985  0.1258  0.0954  0.0489  0.0241  0.0042  0.0086  0.0046  0.0126   \n",
      "207  0.1470  0.0991  0.0041  0.0154  0.0116  0.0181  0.0146  0.0129  0.0047   \n",
      "\n",
      "         54      55      56      57      58      59  \n",
      "0    0.0072  0.0167  0.0180  0.0084  0.0090  0.0032  \n",
      "1    0.0094  0.0191  0.0140  0.0049  0.0052  0.0044  \n",
      "2    0.0180  0.0244  0.0316  0.0164  0.0095  0.0078  \n",
      "3    0.0085  0.0073  0.0050  0.0044  0.0040  0.0117  \n",
      "4    0.0110  0.0015  0.0072  0.0048  0.0107  0.0094  \n",
      "..      ...     ...     ...     ...     ...     ...  \n",
      "203  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157  \n",
      "204  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067  \n",
      "205  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031  \n",
      "206  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048  \n",
      "207  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115  \n",
      "\n",
      "[208 rows x 60 columns]\n",
      "0      R\n",
      "1      R\n",
      "2      R\n",
      "3      R\n",
      "4      R\n",
      "      ..\n",
      "203    M\n",
      "204    M\n",
      "205    M\n",
      "206    M\n",
      "207    M\n",
      "Name: 60, Length: 208, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Separating the features (independent variables) and the target (dependent variable).\n",
    "# X contains all the columns except column 60, which are the features used for prediction.\n",
    "# Y contains column 60, which is the target variable (e.g., rock or mine classification).\n",
    "# 'drop(columns=60, axis=1)' removes column 60 from the dataset to create X.\n",
    "X=df.drop(columns=60,axis=1)\n",
    "\n",
    "# Y is set to column 60, which contains the labels or target variable for classification.\n",
    "Y=df[60]\n",
    "\n",
    "# Printing the features (X) and target (Y) to verify that they have been separated correctly.\n",
    "print(X)  # Displays the feature set\n",
    "print(Y)  # Displays the target variable (classification labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f76de4-7ac2-4dde-8250-25fb06bf9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_histograms(df):\n",
    "    \"\"\"\n",
    "    Plots histograms for all columns in the DataFrame except the last one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the dataset.\n",
    "\n",
    "    Functionality:\n",
    "    - Excludes the last column from the plotting process.\n",
    "    - Creates a histogram for each column with Kernel Density Estimate (KDE) to visualize the data distribution.\n",
    "    - Arranges histograms in a grid with 5 plots per row.\n",
    "    - Dynamically calculates the number of rows required based on the number of columns.\n",
    "    - Any unused subplot spaces are removed for neatness.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exclude the last column from plotting\n",
    "    columns_to_plot = df.columns[:-1]\n",
    "    n_columns = len(columns_to_plot)\n",
    "    \n",
    "    # Calculate the number of rows needed for the grid (5 histograms per row)\n",
    "    n_rows = math.ceil(n_columns / 5)\n",
    "    \n",
    "    # Set up the figure and axes for the plots\n",
    "    fig, axes = plt.subplots(n_rows, 5, figsize=(20, 4*n_rows))  # Dynamic figure size\n",
    "    fig.suptitle('Histograms of all columns (except the last)', fontsize=16)  # Title for the entire plot grid\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot a histogram for each column in the DataFrame, except the last one\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        sns.histplot(data=df, x=column, ax=axes[i], kde=True)  # Plot histogram with KDE\n",
    "        axes[i].set_title(f'Column {column}')  # Set the title of each subplot to indicate the column number\n",
    "        axes[i].set_xlabel('')  # Remove x-axis labels to save space and keep the layout clean\n",
    "    \n",
    "    # Remove any unused subplots (in case the grid size exceeds the number of columns)\n",
    "    for i in range(n_columns, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # Adjust layout to prevent overlap of subplots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the histograms\n",
    "    plt.show()\n",
    "\n",
    "# Calling the function to plot histograms for the DataFrame 'df'\n",
    "plot_all_histograms(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6da3f-be24-4a54-8ab4-a7844a6250d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test sets.\n",
    "# X: Features (independent variables)\n",
    "# Y: Target (dependent variable)\n",
    "# test_size=0.1: Reserves 10% of the data for testing, and 90% for training.\n",
    "# stratify=Y: Ensures that the class distribution in Y is preserved in both the training and test sets.\n",
    "# random_state=1: Ensures reproducibility by setting a seed for the random number generator.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, stratify=Y, random_state=1)\n",
    "\n",
    "# Printing the shapes of the overall dataset, training set, and test set.\n",
    "# This is helpful for verifying that the data was split correctly.\n",
    "print(X.shape, X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c60ea-3431-4d42-b7ab-3fc6ece15900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Logistic Regression model.\n",
    "# Logistic Regression is a linear model commonly used for binary classification tasks.\n",
    "# The model will be trained using the training data (X_train, Y_train) and tested on the test data (X_test, Y_test).\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea361db3-f8aa-4260-9181-bfbbd9e600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Logistic Regression model using the training data.\n",
    "# The fit() method trains the model by finding the optimal weights that map the input features (X_train)\n",
    "# to the target labels (Y_train). This is where the model learns from the training data.\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4029a5-335a-4a43-a447-41b1afbfd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the training data using the trained Logistic Regression model.\n",
    "# The predict() method generates predictions (X_train_prediction) based on the input features (X_train).\n",
    "X_train_prediction = model.predict(X_train)\n",
    "\n",
    "# Calculating the accuracy of the model on the training data.\n",
    "# accuracy_score() compares the predicted labels (X_train_prediction) with the actual labels (Y_train).\n",
    "# It returns the proportion of correct predictions made by the model.\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "\n",
    "# Printing the accuracy of the model on the training data.\n",
    "print(\"The accuracy on training data:\", training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0a637-55e9-447d-bb04-c90aae47c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test data using the trained Logistic Regression model.\n",
    "# The predict() method generates predictions (X_test_prediction) based on the input features (X_test).\n",
    "X_test_prediction = model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model on the test data.\n",
    "# accuracy_score() compares the predicted labels (X_test_prediction) with the actual labels (Y_test).\n",
    "# It returns the proportion of correct predictions made by the model on unseen data (test set).\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
    "\n",
    "# Printing the accuracy of the model on the test data.\n",
    "print(\"The accuracy on test data:\", test_data_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f1def-dd70-40c6-afd5-3426e4628ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    \"\"\"\n",
    "    Applies various normalization techniques to the feature columns of a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame where the last column is the target, \n",
    "                           and the rest are features to be normalized.\n",
    "    \n",
    "    Normalization Techniques Used:\n",
    "    - StandardScaler (Z-score normalization)\n",
    "    - MinMaxScaler\n",
    "    - Log Transformation (logarithmic normalization)\n",
    "    - Square Root Transformation\n",
    "    - Box-Cox Transformation\n",
    "    \n",
    "    The function also plots histograms for each normalized dataset to visualize the effects\n",
    "    of the normalization techniques.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing DataFrames for the original and each normalized version of the features.\n",
    "    - The target column (Y) as a separate series.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.iloc[:, :-1]  # All columns except the last\n",
    "    y = df.iloc[:, -1]   # Last column (target)\n",
    "\n",
    "    # Create copies of features for each normalization technique\n",
    "    X_standard = X.copy()\n",
    "    X_minmax = X.copy()\n",
    "    X_log = X.copy()\n",
    "    X_sqrt = X.copy()\n",
    "    X_boxcox = X.copy()\n",
    "\n",
    "    # StandardScaler (Z-score normalization): Centers the data with mean 0 and variance 1\n",
    "    scaler = StandardScaler()\n",
    "    X_standard[:] = scaler.fit_transform(X)\n",
    "\n",
    "    # MinMaxScaler: Scales data to be within a specified range, usually [0, 1]\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_minmax[:] = minmax_scaler.fit_transform(X)\n",
    "\n",
    "    # Log transformation: Applies logarithmic normalization, adding 1 to handle zero values\n",
    "    X_log[:] = np.log1p(X)\n",
    "\n",
    "    # Square root transformation: Applies square root normalization to reduce the impact of large values\n",
    "    X_sqrt[:] = np.sqrt(X)\n",
    "\n",
    "    # Box-Cox transformation: Applies the Box-Cox normalization technique, adding 1 to handle zero values\n",
    "    for column in X_boxcox.columns:\n",
    "        X_boxcox[column], _ = boxcox(X_boxcox[column] + 1)  # Adding 1 to handle zeros\n",
    "\n",
    "    # Inner function to plot histograms for each normalization technique\n",
    "    def plot_histograms(data, title):\n",
    "        \"\"\"\n",
    "        Plots histograms for each feature in the DataFrame to visualize data distribution.\n",
    "\n",
    "        Parameters:\n",
    "        data (pandas.DataFrame): The DataFrame to plot.\n",
    "        title (str): Title for the plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, col in enumerate(data.columns):\n",
    "            plt.subplot(8, 8, i+1)  # Adjusting for an 8x8 grid layout\n",
    "            sns.histplot(data[col], kde=True)\n",
    "            plt.title(col)\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(title, fontsize=16)  # Main title\n",
    "        plt.subplots_adjust(top=0.95)  # Adjusting space for title\n",
    "        plt.show()\n",
    "\n",
    "    # Plot histograms for the original and normalized datasets\n",
    "    plot_histograms(X, \"Original Data\")\n",
    "    plot_histograms(X_standard, \"StandardScaler Normalization\")\n",
    "    plot_histograms(X_minmax, \"MinMaxScaler Normalization\")\n",
    "    plot_histograms(X_log, \"Log Transformation\")\n",
    "    plot_histograms(X_sqrt, \"Square Root Transformation\")\n",
    "    plot_histograms(X_boxcox, \"Box-Cox Transformation\")\n",
    "\n",
    "    # Return a dictionary containing all the normalized DataFrames and the target variable\n",
    "    return {\n",
    "        'original': X,\n",
    "        'standard': X_standard,\n",
    "        'minmax': X_minmax,\n",
    "        'log': X_log,\n",
    "        'sqrt': X_sqrt,\n",
    "        'boxcox': X_boxcox\n",
    "    }, y\n",
    "\n",
    "# Usage of the function with a given DataFrame 'df'\n",
    "normalized_data, target = normalize_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890726b9-dd97-4a60-a3e9-5909de5af214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(df):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a logistic regression model using the provided DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame where the last column is the target variable\n",
    "                           and the remaining columns are the features.\n",
    "\n",
    "    Functionality:\n",
    "    - Separates the features (X) and target (y).\n",
    "    - Normalizes the features using StandardScaler to ensure the model performs better.\n",
    "    - Splits the dataset into training (80%) and testing (20%) sets.\n",
    "    - Trains a Logistic Regression model on the training data.\n",
    "    - Evaluates the model by calculating accuracy on both the training and testing sets.\n",
    "    \n",
    "    Returns:\n",
    "    - model (LogisticRegression): The trained logistic regression model.\n",
    "    - train_accuracy (float): The accuracy score on the training data.\n",
    "    - test_accuracy (float): The accuracy score on the testing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.iloc[:, :-1]  # All columns except the last one\n",
    "    y = df.iloc[:, -1]   # Last column is the target variable\n",
    "    \n",
    "    # Normalize the features using StandardScaler to standardize the dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the normalized data into training and testing sets (80% training, 20% testing)\n",
    "    # random_state=42 ensures the split is reproducible\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the logistic regression model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy on the training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    # Calculate accuracy on the testing data\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the accuracy scores for training and testing sets\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Return the trained model along with the training and testing accuracy scores\n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "# Assuming your DataFrame is named 'df', you can call the function as follows:\n",
    "model, train_accuracy, test_accuracy = train_and_evaluate_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c50813-a092-4af2-a91f-819cab600920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(df):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a logistic regression model after applying the Box-Cox transformation\n",
    "    to the feature columns of the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame where the last column is the target variable\n",
    "                           and the remaining columns are the features.\n",
    "\n",
    "    Functionality:\n",
    "    - Separates the features (X) and target (y).\n",
    "    - Applies Box-Cox transformation to normalize the distribution of the features.\n",
    "    - Splits the dataset into training (80%) and testing (20%) sets.\n",
    "    - Trains a Logistic Regression model on the transformed training data.\n",
    "    - Evaluates the model by calculating accuracy on both the training and testing sets.\n",
    "    \n",
    "    Returns:\n",
    "    - model (LogisticRegression): The trained logistic regression model.\n",
    "    - train_accuracy (float): The accuracy score on the training data.\n",
    "    - test_accuracy (float): The accuracy score on the testing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.iloc[:, :-1]  # All columns except the last\n",
    "    y = df.iloc[:, -1]   # Last column is the target variable\n",
    "\n",
    "    # Apply Box-Cox transformation to normalize the features\n",
    "    X_boxcox = pd.DataFrame()\n",
    "    for column in X.columns:\n",
    "        # Add a small constant to make all values positive, ensuring no zero or negative values for Box-Cox\n",
    "        min_val = X[column].min()\n",
    "        if min_val <= 0:\n",
    "            X[column] = X[column] - min_val + 1e-5  # Adding a small constant to avoid negative values\n",
    "\n",
    "        # Apply Box-Cox transformation\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')  # Suppress warnings during the transformation\n",
    "            X_boxcox[column], _ = boxcox(X[column])\n",
    "\n",
    "    # Split the transformed data into training (80%) and testing (20%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_boxcox, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the logistic regression model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy on the training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    # Calculate accuracy on the testing data\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the accuracy scores for training and testing sets\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Return the trained model along with the training and testing accuracy scores\n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "# Assuming your DataFrame is named 'df', you can call the function as follows:\n",
    "model, train_accuracy, test_accuracy = train_and_evaluate_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e5dbc-387a-47f6-94c4-3069ca68079d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
